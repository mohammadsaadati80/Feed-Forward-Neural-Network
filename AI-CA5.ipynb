{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf02f54f",
   "metadata": {},
   "source": [
    "# AI Computer Assignment 5 Phase 1 (Feed Forward Neural Network)\n",
    "Mohammad Saadati - \n",
    "_810198410_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cdf740",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project, we implement **Feed Forward neural networks** to classify images.\n",
    "\n",
    "In Feed Forward neural networks, each image is first flattened and given as a vector as Feed Forward in the input neural networks. Each element of this vector (equivalent to one pixel of the image) is a property for it. Based on these properties and by making nonlinear combinations of them, the network is supposed to adjust the weight of the connections between its layers so that With the least error, correctly predict the corresponding input image class.\n",
    "\n",
    "In this assignment we will work with a set of `Persian` handwritten numbers. The data set includes images from 10 digits 0 to 9 in Persian. There are a total of 102352 images in this dataset. You can read more about the data set and the number of images of each digit in the data set [here](http://farsiocr.ir/%D9%85%D8%AC%D9%85%D9%88%D8%B9%D9%87-%D8%AF%D8%A7%D8%AF%D9%87/%D9%85%D8%AC%D9%85%D9%88%D8%B9%D9%87-%D8%A7%D8%B1%D9%82%D8%A7%D9%85-%D8%AF%D8%B3%D8%AA%D9%86%D9%88%DB%8C%D8%B3-%D9%87%D8%AF%DB%8C/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78249482",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "In this part, some of the necessary libraries were imported in order to use their helpful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9097b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f7d9c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl_file = open(\"./dataset/data.pkl\", 'rb')\n",
    "data = pickle.load(data_pkl_file)\n",
    "\n",
    "labels_pkl_file = open(\"./dataset/labels.pkl\", 'rb')\n",
    "labels = pickle.load(labels_pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88691246",
   "metadata": {},
   "source": [
    "## Phase 1: Data review and preprocessing\n",
    "Data splitting is an important phase, If we use whole data for both train and test our mesurments may not be precise,\n",
    "because there may be overfitting on train data.\n",
    "So we can split our data to train/test to handle this challenge, but again there is another problem.\n",
    "First of all we have to divide our dataset to two parts trainset and testset so by using `train_test_split` method of `sklearn` and setting `test_size` we'll determine what percentage of our data is for test. For the division `train_test_split` fucntions from `sklearn.model_selection` is used which returns 4 dataset likes. Two are for the training data and two are for test data. And the dependent variable which here is the `type` feature.\n",
    "\n",
    "The `P` ratio depends on lots of metrics but two important metrics to mention are the number of rows in our data and our data itself.\n",
    "Assume that we have extremely small data in this situations small p may effect our learning accuracy.\n",
    "In larger datasets we can do the split with more confidence. After reading some documents online I came to the understanding that 10% to 30% is a good percentage for test size and by experimenting different values I decided to use the value 20%.\n",
    "\n",
    "Another important note to mention is randomness. We must shuffle our data so there is no learning from the sequence of data itself.\n",
    "`random_state` is for getting the same set of data each time so that we don't get different results, meaning it won't give us random data each time. `random_state` is like the seed that we set for random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7539e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data , labels , test_size = 0.20 , random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c9b3f",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "First, we randomly check and display an image in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4a680615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum value in image: 0\n",
      "Maximum value in image: 255\n"
     ]
    }
   ],
   "source": [
    "image = X_train[random.randint(0 , len(X_train) - 1)]\n",
    "\n",
    "plt.figure(figsize = (2,2))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Random Image From data\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMinimum value in image: \" + str(np.amin(image)))\n",
    "print(\"Maximum value in image: \" + str(np.amax(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14ada8",
   "metadata": {},
   "source": [
    "As shown above image values in the dataset which represent the brightness of each pixel is a number between 0 to 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709eb228",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Now resize the images to $25 * 25$. To do this, we use the `resize` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c9bf42b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum value in image: 0\n",
      "Maximum value in image: 255\n"
     ]
    }
   ],
   "source": [
    "X_train = [np.resize(element,(25 , 25)) for element in X_train]\n",
    "X_test = [np.resize(element,(25 , 25)) for element in X_test]\n",
    "\n",
    "image = X_train[random.randint(0 , len(X_train) - 1)]\n",
    "\n",
    "plt.figure(figsize = (2 , 2))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Random Image From data\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMinimum value in image: \" + str(np.amin(image)))\n",
    "print(\"Maximum value in image: \" + str(np.amax(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018202b",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "In this part, we select and display an image of each class in the train data set. For each image, we display its type along with the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1cd5ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 0 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAACcCAYAAACp45OYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAIeElEQVR4nO3dXYhcdxnH8e+v6UtsasD0JaxttEWSYi1thGDVikQkGHMjXgitIF5UImpBwZtWLyxe9UK9sggVS1E0ImqJaDHWoogg2hTStbVms5bWblOaRqU2FmobHy/mbBynM9mzz3mdmd8Hltk5OzPnyfDkf575z/k/RxGB2Xqd03UANp2cOJbixLEUJ46lOHEsxYljKU4cS3HiVCBpi6T7JP1L0lOSPtp1TG05t+sAptxdwL+BrcBO4GeSHomIxzqNqgXyzHGOpE3AP4BrI2Kp2PYd4JmIuK3T4FrgQ1XeDuD0atIUHgHe1lE8rXLi5F0EvDCy7QXg9R3E0jonTt4pYPPIts3Aix3E0jonTt4ScK6k7UPbrgdmvjAGF8eVSPo+EMAnGHyquh949zx8qvKIU82ngdcBJ4ADwKfmIWnAI44lecSxFCeOpThxLKVS4kjaK+mopGVJMz/Nbv+TLo4lbWAwl7EHWAEeAm6OiD/VF571VZVvx98BLEfEE3BmTuNDwMTEOV8XxEY2VdjleDuue6n218xaWrywk/029R48vPjyyYi4dHR7lcS5HHh66P4KcMPZnrCRTdyg91fY5XiHDh2p/TWzPvDGnZ3st6n3YMPC8lPjtldJHI3Z9prjnqT9wH6AjXTzv9HqV6U4XgG2Dd2/Ajg++qCIuDsidkXErvO4oMLurE+qjDgPAdslXQU8A9wEnPXUyR3XvdTIkNrU4eHQ8SOtPKcr5d635bFb04kTEa9KuhU4BGwA7pmX72ms4jnHEXE/g2+Ebc545thSnDiW0urymKXFC/+vIMsUkuMKutHXKVP0tVXENhVLV/NFqzziWIoTx1KcOJYydUuAx9UDddQRXdYMo/suU/OUeUyT/yaPOJbixLEUJ46ltLo8ZrO2RBPn42TUMY9TZk6pzHO6UuY92LCw/HBE7Brd7hHHUpw4llLp47ikJxl0ZzgNvDpuSLPZVMc8zvsi4mQNr2NTpFJxXIw4u8omzq7rN8YfDm1b+4FDMpNj01aAdqXM+/TL+GEjxXEAv5D0cHFSus2JqoeqGyPiuKTLgAck/TkifjP8gOFVDm+6fOq+4bAJKo04EXG8uD0B3Mdgkd7oY86scrj04g1Vdmc9UmUJ8CbgnIh4sfj9AeDLEfHzSc+pYwKwqZohM5lX134y2qqdJk0AVjl2bAXuk7T6Ot87W9LYbKmyPOYJBs0SbQ555thSnDiW0unn4z5NjvUpllF9jM0jjqU4cSzFiWMprdY489DmpK3XbO+L3PFtTjziWIoTx1KcOJYydascpq9GaEdTcz1e5WC1cuJYypqJI+keSSckPTq0bYukByQdK27f0GyY1jdlRpx7gb0j224DHoyI7cCDxX2bI6WKY0lXAj+NiGuL+0eB3RHxrKQF4NcRcfVar1PHKoc29fHLxUmaep/qXuWwNSKeBShuL6sSnE2fxotjSfslHZZ0+Pm/nW56d9aSbOI8VxyiKG5PTHqgVznMpuyXnD8BPg7cWdwerC2iEdNUZ0Cu1qijXW2mxV25Nifjt5f5OH4A+B1wtaQVSbcwSJg9ko4xuELenWtGYDNlzREnIm6e8Kd+dEiyTnjm2FKmbjH3rLa4X6+mLkPw2tfxiVxWIyeOpThxLMWJYylTdwZgVlN9jaeZ+xxb65w4luLEsZRWa5wyJ3K5Pe1rZdvM1fG+NNWu1uaUE8dSsqsc7pD0jKQjxc++ZsO0vlmzxpH0XuAU8O2hk9XvAE5FxFfWs7M+reRsSlv1Vltte9M1TtEp/e+1RWYzoUqNc6ukxeJQNnFB3vDJ6q/wcoXdWZ9kE+cbwFuAncCzwFcnPXD4ZPXzuCC5O+ubVOJExHMRcToi/gN8kzHXcLDZljoDUNLC6oI84MPAo2d7/Ko+t3Krq9jMXMy1TwX/aCyTVjmsmTjFKofdwCWSVoAvAbsl7WRwvaongU/mQ7VplF3l8K0GYrEp4pljS5m6VQ516aquaPN6W/XwKgerkRPHUpw4ljITNc48nog++m9ue/WqRxxLceJYihPHUpw4ltJqcby0eOFUFaF9+fJx3HvW9fvoEcdSypysvk3SryQ9LukxSZ8ttrst/xwrM+K8Cnw+It4KvBP4jKRrcFv+ubbulZySDgJfL37W1ZY/05K/S03UEX2pm8qqpVtFcU2HtwO/x23551rpxJF0EfAj4HMR8c91PM8t+WdQqcSRdB6DpPluRPy42FyqLb9b8s+mMp+qxOBU0ccj4mtDf1ptyw8Nt+W3/ikzAXgj8DHgj5KOFNu+wKAN/w+KFv1/BT7SRIBdXRth3Os01WM5o71+z+PPACxzsvpvAU34s9vyzynPHFuKE8dSZrJdbVsXtZ+HNnNuV2u1cuJYihPHUjptV9un439dmlhx0WUt5Xa1VisnjqU4cSzFiWMpvZ8AbKstyDxM5mV4AtBq5cSxFCeOpbRa40h6HngKuAQ42dqOq5umeOuO9c0RcenoxlYT58xOpcPjCq6+mqZ424rVhypLceJYSleJc3dH+82apnhbibWTGsemnw9VltJ64kjaK+mopGVJvepwMeH6o71s59J1+5lWE0fSBuAu4IPANcDNRcuUvrgX2Duyra/tXLptPxMRrf0A7wIODd2/Hbi9zRhKxHgl8OjQ/aPAQvH7AnC06xgnxH0Q2NNWvG0fqi4Hnh66v1Js67Pet3Ppov1M24kzbimxP9ZVkG0/U1XbibMCDLfkugI43nIM61WqnUsXqrSfqartxHkI2C7pKknnAzcxaJfSZ71s59J5+5kOirh9wBLwF+CLXReVI7EdYHA57FcYjI63ABcz+HRyrLjd0nWcRazvYXCYXwSOFD/72orXM8eW4pljS3HiWIoTx1KcOJbixLEUJ46lOHEsxYljKf8FV1U/OumbGA8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 1 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 2 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 3 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 4 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 5 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 6 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAACcCAYAAACp45OYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAH1klEQVR4nO3dX4hcZxnH8e+va9rYeNM2f1jTYIskxSJpCotVaiEi0ZibIigmgnhRSC+sWPDCVgVFESpovbAiVFKjoJWilkoprrVYRCo2KSxpatzNtrR2u0vTaMFgVJr4eDEnOoyz2dnnnDnnzM7vA8tkTmb3PF1+ec8zb8/7jiICs9W6pOkCbDQ5OJbi4FiKg2MpDo6lODiW4uBYioNTkqT9kk5I+ruk5yXd0nRNdXhT0wWMMkl7gK8DHwOeBiabrag+8sxxnqSngEMRcajpWurmS1WSpAlgCtgkaV7SgqT7JL256drq4ODkbQHWAR8BbgF2ATcCX2ywpto4OHn/KB6/HRFLEXEauBfY12BNtXFwkiLidWABGMsm0cEp5/vApyVtlnQFcCfwaLMl1cNvx8v5KrARmAP+CTwEfK3Rimrit+OW4kuVpTg4luLgWEqp4EjaK2m2mDm9q6qirP3SzXEx5T4H7KEzn3EEOBARf6yuPGurMm/H3wXMR8QLAJJ+AtwKLBucS3VZrGdDiVOOhx07z676e+aOXT6ESuAMr5+OiE29x8sEZyvwctfzBeCmi33DejZwk95f4pTjYXp6ZtXf88G37qq8DoBfx09f6ne8THDU59j/XfckHQQOAqxnOP8qrH5lmuMFYFvX86uBxd4XRcT9ETEVEVPruKzE6axNyow4R4Dtkq4FXgH2Ax+vpKoRMb04U8t5hnUZKiMdnIg4J+kOYBqYAB6IiOcqq8xardT/5IyIx4DHKqrFRohnji3FwbGUNXk/Tl1N6yDa2NhWwSOOpTg4luLgWErrehz3J/UZ5Hc9scyiZo84luLgWIqDYym19jg7dp5N3WuykrXei/TTdC/oEcdSHBxLKXWpkvQicAY4D5yLiKkqirL2q6LHeV+xxYeNkVqb47ljl49lI9utzqa2mt/1fN+jZXucAH4l6ZnipnQbE2VHnJsjYlHSZuBxSX+KiN92v8CrHNamUiNORCwWj6eAh+ks0ut9jVc5rEHpEUfSBuCSiDhT/PkDwFcqq6yFxnlVQ68yl6otwMOSLvycH0fELyupylqvzPKYF4AbKqzFRohnji3FwbGU1t0BOCxubKvlEcdSHBxLcXAsZeR6nGH1KuPSm1TFI46lODiW4uBYSqM9jvuVZnklp9XOwbGUFYMj6QFJpyQd7zp2paTHJZ0sHq8YbpnWNoOMOIeBvT3H7gKeiIjtwBPFcxsjA30IiKRrgEcj4p3F81lgd0QsSZoEnoyI61b6OVM3rI+np7et9LI1pclGvYo3HxOT88/0Wy+X7XG2RMQSQPG4uUxxNnqG3hxLOijpqKSjr/3l/LBPZzXJBufV4hJF8XhquRd2r3LYdNVE8nTWNtkJwF8AnwTuKR4fqayiFmvTxGLrtzmR9CDwe+A6SQuSbqMTmD2STtL5hLx7hlumtc2KI05EHFjmr/yJZWPMM8eWMnI3clWlTf1Kr6b7l0F4xLEUB8dSHBxLcXAsZeSa4zY3tVUZ5L+x6QbaI46lODiW4uBYirerrZm3q7Wx5uBYSnaVw5clvSJppvjaN9wyrW0G6XEOA/cBP+w5/q2I+EblFbWYd/X6nxVHnGKn9L/WUIuNkDI9zh2SjhWXsmUX5HXfrP4G/ypxOmuTbHC+C7wd2AUsAd9c7oXekn9tSgUnIl6NiPMR8W/ge/T5DAdb21ITgJImLyzIAz4MHL/Y69vAjW21VgxOscphN7BR0gLwJWC3pF10Pq/qReD24ZVobZRd5XBoCLXYCPHMsaW0/kYu9ybt5BHHUhwcS3FwLKXWHmfHzrNMT89U/nPdn/RXzY5c/Y97xLEUB8dSHBxLcXAspdEJQDe1/bVr0tOrHKxCg9ysvk3SbySdkPScpM8Ux70t/xgbZMQ5B3w2It4BvBv4lKTr8bb8Y22Q2yqW6NweSkSckXQC2ArcSuc+HYAfAE8Cn7vYzxqHlZzt6k+GZ1U9TvGZDjcCf8Db8o+1gYMj6S3Az4A7I+Jvq/g+r3JYgwYKjqR1dELzo4j4eXF4oG35vcphbRrkXZXo3Cp6IiLu7fqrC9vywxhty28dg0wA3gx8AnhW0kxx7PN0tuF/qNii/8/AR4dSYU3GpamtyiDvqn4HaJm/9rb8Y8ozx5bi4FhK61c5DMIffF8/jziW4uBYioNjKY32OKO3detoy/y+vcrBKuXgWIqDYykOjqW0fgmwm9r+6pv09CoHq5CDYykOjqUoIuo7mfQa8BKwEThd24nLG6V6q671bRGxqfdgrcH570mloxExVfuJk0ap3rpq9aXKUhwcS2kqOPc3dN6sUaq3llob6XFs9PlSZSm1B0fSXkmzkuYltWqHi2U+f7SV27k0vf1MrcGRNAF8B/gQcD1woNgypS0OA3t7jrV1O5dmt5+JiNq+gPcA013P7wburrOGAWq8Bjje9XwWmCz+PAnMNl3jMnU/Auypq966L1VbgZe7ni8Ux9qs9du5NLH9TN3B6beU2G/rSshuP1NW3cFZALZ1Pb8aWKy5htUaaDuXJpTZfqasuoNzBNgu6VpJlwL76WyX0mat3M6l8e1nGmji9gFzwPPAF5puKntqe5DOfodv0BkdbwOuovPu5GTxeGXTdRa1vpfOZf4YMFN87aurXs8cW4pnji3FwbEUB8dSHBxLcXAsxcGxFAfHUhwcS/kPK4dHyE0uRDsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 7 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 8 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CLASS No. 9 ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    index = 0\n",
    "    while Y_train[index] != i:\n",
    "        index += 1\n",
    "    image = X_train[index]\n",
    "    print(\"========== CLASS No.\",str(i),\"==========\")\n",
    "    plt.figure(figsize = (2 , 2))\n",
    "    plt.imshow(image)\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4663d8",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Now calculate the number of images in each category for the test and train datasets and draw a bar chart for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "19426c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.hist((Y_train, Y_test), label=['train', 'test'])\n",
    "plt.xlabel('Labels', fontsize = 12)\n",
    "plt.ylabel('Image Counts', fontsize = 15)\n",
    "plt.suptitle('Number of Images Per Class', fontsize = 15)\n",
    "plt.legend()\n",
    "plt.xticks(range(0,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd7b7d",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "In this section, scale the amount of data so that the value of each pixel is between 0 and 1 before giving information to the neural network. \n",
    "\n",
    "Normalization refers to rescaling real valued numeric attributes into the range 0 and 1. Neural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process.Since maximum value in the dataset is 255 it's a good idea to divide every value in the dataset by 255, this will result in each value being between 0 to 1. The reason that this procedure is done is that higher numbers in the network will make it biased meaning that since the difference between values in the range of 0 to 255 is a lot the effect of a value like 1 can easily be vanished comapred to 255 so it's a wise move to scale the values down to make each of them have somewhat of a equal impact in the network. Biased network can result in lower accuracy and all in all a poor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f802b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = np.array(X_train), np.array(X_test), np.array(Y_train), np.array(Y_test)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341eceb",
   "metadata": {},
   "source": [
    "## Phase 2: Completing the incomplete parts of given Code for the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c3832",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "815fefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    '''\n",
    "    This class prepares the dataset for the neural network.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, labels, n_classes, batch_size=None, shuffle=False):\n",
    "        '''\n",
    "        This is the constructor. It gets dataset information and initializes the \n",
    "        Dataloader class fields.\n",
    "            Parameters:\n",
    "                data: features your dataset in pandas.Dataframe format.\n",
    "                labels: labels of your dataset in pandas.Dataframe format.\n",
    "                n_classes: number of classes you have in your dataset.\n",
    "                batch_size: the number of samples that will be propagated through the network.\n",
    "                shuffle: boolean value indicating whether or not the dataset should be shuffled\n",
    "        '''\n",
    "\n",
    "        assert len(data)==len(labels)\n",
    "        self.__n_classes = n_classes\n",
    "        self.__batch_size = batch_size\n",
    "        self.__shuffle = shuffle\n",
    "        self.__data = data\n",
    "        self.__onehot_labels = self.__onehot(labels, self.__n_classes)\n",
    "    \n",
    "    def __onehot(self, labels, n_classes):\n",
    "        '''\n",
    "        This private method gets labels and provides one_hot vectors of labels.\n",
    "        For categorical variables where no such ordinal relationship exists,\n",
    "        the integer encoding is not enough.\n",
    "        In this case, a one-hot encoding can be applied to the integer representation.\n",
    "        This is where the integer encoded variable is removed, and a new binary variable is\n",
    "        added for each unique integer value.\n",
    "        example:\n",
    "            red,    green,    blue\n",
    "            1,      0,        0\n",
    "            0,      1,        0\n",
    "            0,      0,        1\n",
    "                Parameters:\n",
    "                        label: lables of your dataset in pandas.Dataframe format.\n",
    "                        n_classes: number of classes you have in your dataset.\n",
    "                \n",
    "                Returns:\n",
    "                    onehot_vectors: onehot vectors of the labels\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "#         labels = labels.astype(int)\n",
    "#         onehot_vectors = np.zeros((labels.size, n_classes))\n",
    "#         onehot_vectors[np.arange(labels.size), labels] = 1.0\n",
    "#         return onehot_vectors\n",
    "        onehot_vectors = pd.DataFrame(OneHotEncoder().fit_transform(labels).toarray())\n",
    "        return onehot_vectors\n",
    "    \n",
    "    def __shuffle_dataset(self):\n",
    "        '''\n",
    "        This private method shuffles your dataset.\n",
    "        It uses data and onehot_labels to shuffle them\n",
    "        symmetrical.\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        rp = np.random.permutation(len(self.__data))\n",
    "        self.__data = self.__data[rp]\n",
    "        self.__onehot_labels = self.__onehot_labels[rp]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        The __iter__() function returns an iterator for the\n",
    "        given object (array, set, tuple, etc., or custom objects).\n",
    "        This will return your dataset in the batch_size given. This should\n",
    "        be used to provide data for the neural network.\n",
    "        '''\n",
    "        \n",
    "        if self.__shuffle:\n",
    "            self.__shuffle_dataset()\n",
    "            \n",
    "        if self.__batch_size==None:\n",
    "            yield (np.matrix(self.__data), np.matrix(self.__onehot_labels))\n",
    "            return\n",
    "            \n",
    "        for idx in range(0, len(self.__data), self.__batch_size):\n",
    "            yield (np.matrix(self.__data[idx:idx+self.__batch_size]), \n",
    "                   np.matrix(self.__onehot_labels[idx:idx+self.__batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab29b35",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "In this part, we completed _val_ and *derivative* methods of activation function classes.\n",
    "\n",
    "An activation function defines how the weighted sum of the layer input is transformed into an output from a node/s in the network.\n",
    "\n",
    "We used the following formulas to implement these activation functions:\n",
    "\n",
    "$$  Identical(x) = x \\quad \\text{and} \\quad Identical'(x) = 1 $$\n",
    "\n",
    "$$ \n",
    "Relu(x) = \\begin{cases} x & x \\geq 0\\\\\n",
    "0 & x < 0 \\end{cases}  \\quad \\text{and} \\quad \n",
    "Relu'(x) = \\begin{cases} 1 & x \\geq 0\\\\\n",
    "0 & x < 0 \\end{cases} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "LeakyRelu(x) = \\begin{cases} x & x \\geq 0\\\\\n",
    "\\text{negative_slope}*x & x < 0 \\end{cases} \\quad \\text{and} \\quad \n",
    "LeakyRelu'(x) = \\begin{cases} 1 & x \\geq 0\\\\\n",
    "\\text{negative_slope} & x < 0 \\end{cases} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "Sigmoid(x) = \\frac{1}{1+e^{-x}} \\quad \\text{and} \\quad \n",
    "Sigmoid'(x) = Sigmoid(x)(1-Sigmoid(x)) \n",
    "$$\n",
    "\n",
    "$$ \n",
    "Softmax(x : matrix) = \\frac{e^x_i}{\\sum_{j=1}^{J} e^x_j}  \\quad for \\quad i = 1,...,Z \\quad and \\quad x = (x_1, x_2,...,x_Z)\n",
    "$$\n",
    "\n",
    "$$ \n",
    "Tanh(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \\quad \\text{and} \\quad \n",
    "Tanh'(x) = 1-Tanh(x)^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "954f6054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identical:\n",
    "    '''\n",
    "    This is the Identical activation function. This activation function just\n",
    "    return the value it gets.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        This is the constructor. It does not have any fields\n",
    "        as a result, there is no need to do anything in the constructor.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def __val(self, matrix):\n",
    "        '''\n",
    "        This private method gets a matrix and uses the activity function on that.\n",
    "        As this is an identical activity function, it just \n",
    "        returns np.matrix of the input.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                identical_value: np.matrix of input with float datatypes\n",
    "        '''\n",
    "        identical_value = np.matrix(matrix, dtype=float)\n",
    "        return identical_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        '''\n",
    "        This method returns the derivation of the input.\n",
    "        As the derivation of x is one, this method returns\n",
    "        a matrix of one with the shape of the input matrix.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                identical_derivative: np.matrix of ones with matrix shape\n",
    "        '''\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        identical_derivative = np.matrix(np.full(np.shape(temp), 1.))\n",
    "        return identical_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        '''\n",
    "        __call__ is a special function in Python that, when implemented inside a class,\n",
    "        gives its instances (objects) the ability to behave like a function.\n",
    "        Here we return the _value method output.\n",
    "            \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                __val(matrix): __val return value for the input matrix\n",
    "        '''\n",
    "        return self.__val(matrix)\n",
    "    \n",
    "\n",
    "class Relu:\n",
    "    '''\n",
    "    This is the Relu activation function. \n",
    "    The rectified linear activation function or ReLU for short\n",
    "    is a piecewise linear function that will output the input directly\n",
    "    if it is positive, otherwise, it will output zero.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        '''\n",
    "        This is the constructor. It does not have any fields\n",
    "        as a result, there is no need to do anything in the constructor.\n",
    "        '''\n",
    "    \n",
    "    def __val(self, matrix):\n",
    "        '''\n",
    "        This private method gets a matrix and uses the activity function on that.\n",
    "        It will set 0 in the matrix if the value is less than 0 else, it returns the value itself.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                relu_value: np.matrix of relu activation function result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        relu_value = np.matrix(np.maximum(temp, 0.))\n",
    "        return relu_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        '''\n",
    "        Returns the derivation value of relu function on input matrix.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                relu_derivative: np.matrix of relu activation function derivation result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        relu_derivative = np.matrix(matrix, dtype=float)\n",
    "        relu_derivative[relu_derivative < 0] = 0.0\n",
    "        relu_derivative[relu_derivative >= 0] = 1.0\n",
    "        return relu_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        '''\n",
    "        __call__ is a special function in Python that, when implemented inside a class,\n",
    "        gives its instances (objects) the ability to behave like a function.\n",
    "        Here we return the _relu method output.\n",
    "            \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                __relu(matrix): __relu return value for the input matrix\n",
    "        '''\n",
    "        return self.__val(matrix)\n",
    "\n",
    "    \n",
    "class LeakyRelu:\n",
    "    '''\n",
    "    This is the Leaky Relu activation function. \n",
    "    Leaky Rectified Linear Unit, or Leaky ReLU,\n",
    "    is a type of activation function based on a ReLU,\n",
    "    but it has a small slope for negative values instead\n",
    "    of a flat slope.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        '''\n",
    "        This is the constructor.\n",
    "        It sets negative_slope field.\n",
    "            Parameters:\n",
    "                negative_slope: slope for negative input values\n",
    "        '''\n",
    "        self.negative_slope = 0.01\n",
    "    \n",
    "    def __val(self, matrix):\n",
    "        '''\n",
    "        This private method gets a matrix and uses the activity function on that.\n",
    "        It will set negative_slope*value in the matrix if the value is less than 0, else it\n",
    "        returns the value itself.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                relu_value: np.matrix of relu activation function result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        leacky_relu_value = np.matrix(np.where(temp >= 0, temp, temp * self.negative_slope))\n",
    "        return leacky_relu_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        '''\n",
    "        Returns the derivation value of leaky relu function on input matrix.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                leacky_relu_derivative: np.matrix of leaky relu activation function derivation result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "#         leaky_relu_derivative = 1.0\n",
    "#         leaky_relu_derivative[temp < 0.] = self.negative_slope\n",
    "        leaky_relu_derivative = np.matrix(np.full(np.shape(temp), np.where(temp < 0, self.negative_slope, 1.0)))\n",
    "        return leaky_relu_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        '''\n",
    "        __call__ is a special function in Python that, when implemented inside a class,\n",
    "        gives its instances (objects) the ability to behave like a function.\n",
    "        Here we return the _val method output.\n",
    "            \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                __val(matrix): __val return value for the input matrix\n",
    "        '''\n",
    "        return self.__val(matrix)\n",
    "\n",
    "    \n",
    "class Sigmoid:\n",
    "    '''\n",
    "    A sigmoid function is a mathematical function having a\n",
    "    characteristic \"S\"-shaped curve or sigmoid curve.\n",
    "    It return S(x)=1/(1+e^-x)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self): \n",
    "        pass\n",
    "\n",
    "    def __val(self, matrix):\n",
    "        '''\n",
    "        Returns 1/(1+e^-x) of values\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                sigmoid_value: np.matrix of relu activation function result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        sigmoid_value = np.matrix(1.0/(1.0 + np.exp(-temp)))\n",
    "        return sigmoid_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        '''\n",
    "        Returns the derivation value of sigmoid function on input matrix.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                sigmoid_derivative: np.matrix of sigmoid activation function derivation result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        sigmoid_derivative = np.multiply(self.__val(temp), (1. - self.__val(temp)))\n",
    "        return sigmoid_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        '''\n",
    "        __call__ is a special function in Python that, when implemented inside a class,\n",
    "        gives its instances (objects) the ability to behave like a function.\n",
    "        Here we return the _val method output.\n",
    "            \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                __val(matrix): __val return value for the input matrix\n",
    "        '''\n",
    "        return self.__val(matrix)\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    '''\n",
    "    The softmax function, also known as softargmax  or normalized\n",
    "    exponential function is a generalization of the logistic\n",
    "    function to multiple dimensions. It is used in multinomial logistic\n",
    "    regression and is often used as the last activation function of a neural\n",
    "    network to normalize the output of a network to a probability distribution\n",
    "    over predicted output classes, based on Luce's choice axiom.\n",
    "    Softmax return (e^x_i / (Σe^x_j for j = 1, ..., J))\n",
    "    '''\n",
    "        \n",
    "    def __init__(self): \n",
    "        pass\n",
    "        '''\n",
    "        This is the constructor. It does not have any fields\n",
    "        as a result, there is no need to do anything in the constructor.\n",
    "        '''\n",
    "\n",
    "    def __val(self, matrix):\n",
    "        '''\n",
    "        This private method gets a matrix and uses the softmax on that.\n",
    "        Softmax return (e^x_i / (Σe^x_j for j = 1, ..., J))\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                relu_value: np.matrix of relu activation function result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        softmax_value = np.exp(temp - np.max(temp))\n",
    "        softmax_value /= softmax_value.sum(axis=1)\n",
    "        return softmax_value\n",
    "\n",
    "#         temp = np.matrix(matrix, dtype=float)    \n",
    "#         softmax_value = np.apply_along_axis(lambda row: np.exp(row - np.max(row))/np.sum(np.exp(row - np.max(row))), 1, temp)\n",
    "#         return softmax_value\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        '''\n",
    "        __call__ is a special function in Python that, when implemented inside a class,\n",
    "        gives its instances (objects) the ability to behave like a function.\n",
    "        Here we return the _val method output.\n",
    "            \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                __val(matrix): __val return value for the input matrix\n",
    "        '''\n",
    "        return self.__val(matrix)\n",
    "    \n",
    "class Tanh:\n",
    "    \n",
    "    def __init__(self): \n",
    "      '''\n",
    "      This is the constructor. It does not have any fields\n",
    "      as a result, there is no need to do anything in the constructor.\n",
    "      '''\n",
    "      pass\n",
    "\n",
    "    def __val(self, matrix):\n",
    "        '''\n",
    "        This private method gets a matrix and uses the activity function on that.\n",
    "        It performs Tanh on the values.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                tanh_value: np.matrix of Tanh activation function result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        tanh_value = np.matrix((np.exp(temp) - np.exp(-temp))/(np.exp(temp) + np.exp(-temp)))\n",
    "        return tanh_value\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        '''\n",
    "        Returns the derivation value of Tanh function on input matrix.\n",
    "        \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                sigmoid_derivative: np.matrix of Tanh activation function derivation result\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        tanh_derivative = 1. - np.power(self.__val(temp), 2)\n",
    "        return tanh_derivative\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        '''\n",
    "        __call__ is a special function in Python that, when implemented inside a class,\n",
    "        gives its instances (objects) the ability to behave like a function.\n",
    "        Here we return the _val method output.\n",
    "            \n",
    "            Parameters:\n",
    "                matrix: np.matrix of values\n",
    "            Returns:\n",
    "                __val(matrix): __val return value for the input matrix\n",
    "        '''\n",
    "        return self.__val(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab41bd",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "In this part, we completed _val_ and *derivative* methods of CrossEntropy classes. It is assumed that `Sofmax` is applied before using the following formulas.\n",
    "$$ CrossEntropy(y, \\hat y) = -\\sum_{i} y_i \\log{\\hat y_i} $$\n",
    "\n",
    "$$ CrossEntropy'(y, \\hat y) = y - \\hat y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0d33eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy: #(with softmax)\n",
    "    '''\n",
    "    Cross-entropy is a measure of the difference between two probability\n",
    "    distributions for a given random variable or set of events. You might\n",
    "    recall that information quantifies the number of bits required to encode\n",
    "    and transmit an event.\n",
    "    The above image can help you.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self): \n",
    "        pass\n",
    "        '''\n",
    "        This is the constructor. It does not have any fields\n",
    "        as a result, there is no need to do anything in the constructor.\n",
    "        '''\n",
    "\n",
    "    def __val(self, true_val, expected_val):\n",
    "        '''\n",
    "        L(y^, y) = - Σ (y^(k)log (y^)^(k)) for k in K\n",
    "        Parameters:\n",
    "            true_val: calculated values (generated by neural network)\n",
    "            expected_val: real values in dataset\n",
    "        Returns:\n",
    "            cross_entropy_value: cross entropy of inputs\n",
    "        '''\n",
    "        assert np.shape(true_val)==np.shape(expected_val)\n",
    "        # TODO: Implement\n",
    "#         softmax = Softmax()\n",
    "#         cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), \\\n",
    "#                                                   np.log(softmax(true_val))), axis=1)\n",
    "#         return cross_entropy_value\n",
    "    \n",
    "        true_val_mat = np.matrix(true_val, dtype=float)\n",
    "        cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
    "        return cross_entropy_value\n",
    "        \n",
    "    def derivative(self, true_val, expected_val):\n",
    "        '''\n",
    "        Returns derivation of cross entropy.\n",
    "            Parameters:\n",
    "                true_val: calculated values (generated by neural network)\n",
    "                expected_val: real values in dataset\n",
    "            Returns:\n",
    "                cross_entropy_derivative: cross entropy derivation of inputs\n",
    "        '''\n",
    "        assert np.shape(true_val)==np.shape(expected_val)\n",
    "        # TODO: Implement\n",
    "#         softmax = Softmax()\n",
    "#         cross_entropy_derivative = softmax(true_val) - expected_val\n",
    "#         return cross_entropy_derivative\n",
    "\n",
    "        true_val_mat = np.matrix(true_val, dtype=float)                          \n",
    "        cross_entropy_derivative = Softmax()(true_val_mat) - np.matrix(expected_val, dtype=float)                                 \n",
    "        return cross_entropy_derivative\n",
    "    \n",
    "    def __call__(self, true_val, expected_val):\n",
    "        '''\n",
    "        __call__ is a special function in Python that, when implemented inside a class,\n",
    "        gives its instances (objects) the ability to behave like a function.\n",
    "        Here we return the _val method output.\n",
    "            \n",
    "            Parameters:\n",
    "                true_val: calculated values (generated by neural network)\n",
    "                expected_val: real values in dataset\n",
    "            Returns:\n",
    "                __val(matrix): __val return value for the input matrix\n",
    "        '''\n",
    "        return self.__val(true_val, expected_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1082b8",
   "metadata": {},
   "source": [
    "### Layer\n",
    "In this part, we completed _forward_, *update_weights*, *uniform_weight*, and *normal_weight* methods of Layer class. \n",
    "\n",
    "We used the following formulas to update weights at the end of each batch:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = x^T\\frac{\\partial L}{\\partial y} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b} = 1\\frac{\\partial L}{\\partial y} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y}W^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9255707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    '''\n",
    "    The layer class is used to define neural network layers.\n",
    "    It stores all needed information for each layer, such as neurons count,\n",
    "    weight matrix, bias, the output after applying the activation function, etc.\n",
    "    '''\n",
    "\n",
    "    DEFAULT_LOW, DEFAULT_HIGH, DEFAULT_MEAN, DEFAULT_VAR = 0, 0.05, 0., 1.\n",
    "  \n",
    "    def __init__(self, input_size, output_size, activation=Identical(), initial_weight='uniform', \n",
    "                 **initializing_parameters):\n",
    "        '''\n",
    "        Parameters:\n",
    "            input_size: the size of the input of this layer.\n",
    "            output_size: the size of the output after this layer.\n",
    "            activation: the activation function. It can be initialized to either of the previously defined activation classes.\n",
    "                               default is an Identical activation function.\n",
    "            initial_weight: either normal or uniform. It defines the method for weight initialization.\n",
    "        '''\n",
    "        \n",
    "        assert type(initial_weight)==str, 'Undefined activation function!'\n",
    "        \n",
    "        self.__weight_initializer_dict = {'uniform':self.__uniform_weight, 'normal':self.__normal_weight}\n",
    "        \n",
    "        assert initial_weight in self.__weight_initializer_dict, 'Undefined weight initialization function!'\n",
    "\n",
    "\n",
    "        self.__n_neurons = output_size\n",
    "        weight_initializer = self.__weight_initializer_dict[initial_weight]\n",
    "        self.__weight = weight_initializer(input_size, self.__n_neurons, **initializing_parameters)\n",
    "        self.__bias = weight_initializer(1, self.__n_neurons, **initializing_parameters)\n",
    "        self.__activation = activation\n",
    "        \n",
    "        self.__last_input = None\n",
    "        self.__last_activation_input = None\n",
    "        self.__last_activation_output = None\n",
    "        self.__last_activation_derivative = None\n",
    "        \n",
    "    def forward(self, layer_input):\n",
    "        '''\n",
    "        It calculates the output of this layer for the layer_input argument.\n",
    "        This method also stores __last_input, __last_activation_input, and __last_activation_derivative\n",
    "        for future use in backpropagation.\n",
    "        Parameters:\n",
    "            layer_input: 2d np.matrix representing the input matrix of this layer.\n",
    "        Returns:\n",
    "            Final output of this layer after applying the activation function.\n",
    "        '''\n",
    "        assert np.ndim(layer_input)==2\n",
    "        assert np.size(self.__weight,0) == np.size(layer_input,1)\n",
    "        # TODO: Implement\n",
    "#         self.__last_input = layer_input\n",
    "#         self.__last_activation_input = np.add(np.dot(self.__last_input, self.__weight), self.__bias)\n",
    "#         self.__last_activation_output = self.__activation(self.__last_activation_input)\n",
    "#         self.__last_activation_derivative = self.__activation.derivative(self.__last_activation_output)\n",
    "#         return self.__last_activation_output\n",
    "        self.__last_input = np.matrix(layer_input, dtype=float)\n",
    "        self.__last_activation_input = np.add(np.dot(self.__last_input, self.__weight), self.__bias)\n",
    "        self.__last_activation_output = self.__activation(self.__last_activation_input)\n",
    "        self.__last_activation_derivative = self.__activation.derivative(self.__last_activation_input)        \n",
    "        return self.__last_activation_output\n",
    "    \n",
    "    def update_weights(self, backprop_tensor, lr):\n",
    "        '''\n",
    "        It updates Layer weights according to the backpropagation matrix and learning rate.\n",
    "        This method updates bias values as well.\n",
    "        Parameters:\n",
    "            backprop_tensor: 2d np.matrix passed from the next layer containing gradient values.\n",
    "            lr: learning rate\n",
    "        Returns:\n",
    "            backprop_tensor to be used by the previous layer.\n",
    "        '''\n",
    "        assert np.ndim(backprop_tensor)==2\n",
    "        assert np.size(backprop_tensor,0) == np.size(self.__last_activation_derivative,0)\n",
    "        assert np.size(backprop_tensor,1) == self.__n_neurons\n",
    "        # TODO: Implement\n",
    "        backprop_tensor = np.multiply(backprop_tensor, self.__last_activation_derivative)\n",
    "        self.__weight -= (lr * np.dot(self.__last_input.T, backprop_tensor))\n",
    "        self.__bias -= (lr * np.dot(np.ones((1, backprop_tensor.shape[0])), backprop_tensor))\n",
    "        backprop_tensor = np.dot(backprop_tensor, self.__weight.T)\n",
    "        return backprop_tensor\n",
    "    \n",
    "#         ones_matrix = np.matrix(np.ones((np.size(backprop_tensor,0), 1)))\n",
    "#         dy = np.multiply(backprop_tensor, self.__last_activation_derivative)\n",
    "#         db = np.matmul(np.transpose(ones_matrix), dy)\n",
    "#         dw = np.matmul(np.transpose(self.__last_input), dy)\n",
    "#         backprop_tensor = np.matmul(dy, np.transpose(self.__weight))\n",
    "#         self.__weight -= lr * dw\n",
    "#         self.__bias -= lr * db\n",
    "#         return backprop_tensor\n",
    "        \n",
    "\n",
    "    def __uniform_weight(self, dim1, dim2, **initializing_parameters):\n",
    "        '''\n",
    "        Initializes weights as a uniform distribution between low and high values.\n",
    "        It uses default low and high values unless low or high are passed in initializing_parameters.\n",
    "        Parameters:\n",
    "            dim1: the size of the first dimension of weights.\n",
    "            dim2: the size of the second dimension of weights.\n",
    "            initializing_parameters: other initializing parameters; it can include custom low or high values.\n",
    "        Returns:\n",
    "            np.matrix with size (dim1, dim2) initialized using uniformly distributed values.\n",
    "        '''\n",
    "        low, high = self.DEFAULT_LOW, self.DEFAULT_HIGH\n",
    "        if 'low' in initializing_parameters.keys(): low = initializing_parameters['low']\n",
    "        if 'high' in initializing_parameters.keys(): high = initializing_parameters['high']\n",
    "        # TODO: Implement\n",
    "#         weights = np.random.uniform(low, high=high, size=(dim1, dim2))\n",
    "        weights = np.matrix(np.random.uniform(low, high, size=(dim1, dim2)))\n",
    "        return weights\n",
    "\n",
    "    def __normal_weight(self, dim1, dim2, **initializing_parameters):\n",
    "        '''\n",
    "        Initializes weights as a normal distribution with mean and var values.\n",
    "        It uses default mean and variance values unless mean or var are passed in initializing_parameters.\n",
    "        Parameters:\n",
    "            dim1: the size of the first dimension of weights.\n",
    "            dim2: the size of the second dimension of weights.\n",
    "            initializing_parameters: other initializing parameters; it can include custom mean or var values.\n",
    "        Returns:\n",
    "            np.matrix with size (dim1, dim2) initialized using normaly distributed values.\n",
    "        ''' \n",
    "        mean, var = self.DEFAULT_MEAN, self.DEFAULT_VAR\n",
    "        if 'mean' in initializing_parameters.keys(): mean = initializing_parameters['mean']\n",
    "        if 'var' in initializing_parameters.keys(): var = initializing_parameters['var']\n",
    "        # TODO: Implement\n",
    "#         weights = np.random.normal(loc=mean, scale=var, size=(dim1, dim2))\n",
    "        weights = np.matrix(np.random.normal(mean, sqrt(var), size=(dim1, dim2)))\n",
    "        return weights\n",
    "    \n",
    "    @property\n",
    "    def n_neurons(self): return self.__n_neurons\n",
    "    \n",
    "    @property\n",
    "    def weight(self): return self.__weight\n",
    "    \n",
    "    @property\n",
    "    def bias(self): return self.__bias\n",
    "    \n",
    "    @property\n",
    "    def activation(self): return self.__activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae398ba",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "307831c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    '''\n",
    "    This class is used in order to implement custom feed-forward neural networks.\n",
    "    The FeedForwardNN class stores a list of layers that determines all network layers.\n",
    "    It also consists of the learning rate and loss function.\n",
    "    '''\n",
    "    def __init__(self, input_shape):\n",
    "        '''\n",
    "        Parameters:\n",
    "            input_shape: the size of the first input to our neural network.\n",
    "        '''\n",
    "        \n",
    "        self.__input_shape = input_shape\n",
    "        self.__output_shape = None\n",
    "        \n",
    "        self.__layers_list = []\n",
    "        \n",
    "        self.__lr = None\n",
    "        self.__loss = None\n",
    "\n",
    "        \n",
    "    def add_layer(self, n_neurons, activation=Relu(), initial_weight='uniform', **initializing_parameters):\n",
    "        '''\n",
    "         This method adds a new custom layer to the layers_list.\n",
    "         Parameters:\n",
    "             n_neurons: number of neurons in this layer\n",
    "             activation: the activation function of this layer, default is Relu\n",
    "             initial_weight: either a uniform or normal, default is uniform\n",
    "             initializing_parameters: other initializing parameters such as low, high, mean, var, etc\n",
    "        '''\n",
    "         \n",
    "        assert type(n_neurons)==int, \"Invalid number of neurons for the layer!\"\n",
    "        assert n_neurons>0, \"Invalid number of neurons for the layer!\"\n",
    "        \n",
    "        n_prev_neurons = self.__input_shape if len(self.__layers_list)==0 else self.__layers_list[-1].n_neurons\n",
    "        new_layer = Layer(n_prev_neurons, n_neurons, activation, initial_weight, **initializing_parameters)\n",
    "        self.__layers_list.append(new_layer)\n",
    "        self.__output_shape = self.__layers_list[-1].n_neurons \n",
    "      \n",
    "    \n",
    "    def set_training_param(self, loss=CrossEntropy(), lr=1e-3):\n",
    "        '''\n",
    "        This method is used to set training parameters.\n",
    "        Parameters:\n",
    "            loss: loss function, default is CrossEntropy\n",
    "            lr: learning rate, default is 1e-3\n",
    "        '''\n",
    "        assert self.__layers_list, \"Uncomplete model!\"\n",
    "        self.__loss = loss\n",
    "        self.__lr = lr\n",
    "    \n",
    "    \n",
    "    def forward(self, network_input):\n",
    "        '''\n",
    "        This method calculates the output of the complete neural network for a passed input.\n",
    "        Parameters:\n",
    "            network_input: input of the neural network\n",
    "        Returns:\n",
    "            network_output: output of the neural network after forwarding the network_input\n",
    "        '''\n",
    "        assert type(self.__output_shape) != None, \"Model is not compiled!\"\n",
    "        # TODO: Implement\n",
    "        network_output = network_input\n",
    "#         for i in range(len(self.__layers_list)):\n",
    "#             network_output = self.__layers_list[i].forward(network_output)\n",
    "#             if i == (len(self.__layers_list)-2):\n",
    "#                 self.__before_last = network_output\n",
    "        for layer in self.__layers_list:\n",
    "            network_output = layer.forward(network_output)\n",
    "        return network_output\n",
    "    \n",
    "    \n",
    "    def fit(self, epochs, trainloader, testloader=None, print_results=True):\n",
    "        '''\n",
    "        This method trains the neural network using specified parameters.\n",
    "        It runs the __train private method epoch times and fills the log dictionary.\n",
    "        Parameters:\n",
    "            epochs: number of epochs to run\n",
    "            trainloader: DataLoader for train data\n",
    "            testloader: DataLoader for test data\n",
    "            print_results: whether or not to print the results\n",
    "        Returns:\n",
    "            log: complete log of the training process as a dictionary consisting of\n",
    "            train_accuracy, train_loss, test_accuracy, test_loss\n",
    "        '''\n",
    "        \n",
    "        assert type(self.__output_shape) != None, \"Model is not compiled!\"\n",
    "        assert type(self.__lr) != None and type(self.__loss) != None, \"Training paramenters are not set!\"\n",
    "\n",
    "        log = {\"train_accuracy\":[], \"train_loss\":[], \"test_accuracy\":[], \"test_loss\":[]}\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            if print_results: \n",
    "                print('Epoch {}:'.format(epoch)) \n",
    "                \n",
    "            average_accuracy, average_loss = self.__train(trainloader)\n",
    "            log['train_accuracy'].append(average_accuracy)\n",
    "            log['train_loss'].append(average_loss)\n",
    "            if print_results:\n",
    "                print('\\tTrain: Average Accuracy: {}\\tAverage Loss: {}'.format(average_accuracy, average_loss))\n",
    "            \n",
    "            if type(testloader) != type(None):\n",
    "                average_accuracy, average_loss = self.__test(testloader)\n",
    "                log['test_accuracy'].append(average_accuracy)\n",
    "                log['test_loss'].append(average_loss)\n",
    "                if print_results:\n",
    "                    print('\\tTest: Average Accuracy: {}\\tAverage Loss: {}'.format(average_accuracy, average_loss))\n",
    "                    \n",
    "        return log\n",
    "    \n",
    "    \n",
    "    def __train(self, trainloader):\n",
    "        '''\n",
    "        Trains the neural network for one epoch.\n",
    "        Parameters:\n",
    "            trainloader: A DataLoader consisting of train data\n",
    "        Returns:\n",
    "            batch_accuracy, batch_loss: mean of all batch_accuracies, batch_losses\n",
    "        '''\n",
    "        bach_accuracies, batch_losses = [], []\n",
    "        for x_train, y_train in trainloader:\n",
    "            batch_accuracy, batch_loss = self.__train_on_batch(x_train, y_train)\n",
    "            bach_accuracies.append(batch_accuracy)\n",
    "            batch_losses.append(batch_loss)\n",
    "        return np.mean(bach_accuracies), np.mean(batch_losses)\n",
    "    \n",
    "    \n",
    "    def __test(self, testloader):\n",
    "        '''\n",
    "        Test the neural network using a testloader.\n",
    "        Parameters:\n",
    "            testloader: A DataLoader of test data\n",
    "        Returns:\n",
    "            batch_accuracy, batch_loss: mean of all batch_accuracies, batch_losses\n",
    "        '''\n",
    "        bach_accuracies, batch_losses = [], []\n",
    "        for x_test, y_test in testloader:\n",
    "            batch_accuracy, batch_loss = self.__test_on_batch(x_test, y_test)\n",
    "            bach_accuracies.append(batch_accuracy)\n",
    "            batch_losses.append(batch_loss)\n",
    "        return np.mean(bach_accuracies), np.mean(batch_losses)\n",
    "\n",
    "    \n",
    "    def __train_on_batch(self, x_batch, y_batch):\n",
    "        '''\n",
    "        Trains the neural network for one batch of train data.\n",
    "        Parameters:\n",
    "            x_batch: one batch data\n",
    "            y_batch: labels for one batch\n",
    "        Returns:\n",
    "            (batch_accuracy, batch_average_loss)\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        network_output = self.forward(x_batch)\n",
    "        batch_accuracy = self.__compute_accuracy(y_batch, network_output)\n",
    "        batch_average_loss = np.mean(self.__loss(network_output, y_batch))\n",
    "        self.__update_weights(network_output, y_batch) \n",
    "        return (batch_accuracy, batch_average_loss)\n",
    "        \n",
    "        \n",
    "    def __test_on_batch(self, x_batch, y_batch):\n",
    "        '''\n",
    "        Tests the neural network for one batch of test data.\n",
    "        Parameters:\n",
    "            x_batch: one batch data\n",
    "            y_batch: labels for one batch\n",
    "        Returns:\n",
    "            (batch_accuracy, batch_average_loss)\n",
    "        '''  \n",
    "        # TODO: Implement\n",
    "        network_output = self.forward(x_batch)\n",
    "        batch_accuracy = self.__compute_accuracy(network_output, y_batch)\n",
    "        batch_average_loss = np.mean(self.__loss(network_output, y_batch))\n",
    "        return (batch_accuracy, batch_average_loss)\n",
    "            \n",
    "        \n",
    "    def __get_labels(self, outputs):\n",
    "        '''\n",
    "        Parameters:\n",
    "            outputs: output of the neural network\n",
    "        Returns:\n",
    "            labels: labels generated from the outputs of the neural network\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        labels = np.argmax(outputs, axis = 1)\n",
    "        return labels\n",
    "    \n",
    "    \n",
    "    def __compute_accuracy(self, output, expected_output):\n",
    "        '''\n",
    "        Computes accuracy by comparing output and expected_output.\n",
    "        Parameters:\n",
    "            output: actual output of the neural network\n",
    "            expected_output: expected output\n",
    "        Returns:\n",
    "            accuracy\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "#         accuracy = ((self.__get_labels(output) == self.__get_labels(expected_output)).sum()/output.shape[0]) * 100\n",
    "#         return accuracy\n",
    "    \n",
    "        accuracy = np.count_nonzero(self.__get_labels(output) == self.__get_labels(expected_output)) / np.size(output, 0) * 100\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def __update_weights(self, output, y_train):\n",
    "        '''\n",
    "        Updates weights of all layers according to neural network output and labels.\n",
    "        Parameters:\n",
    "            output: output of the neural network\n",
    "            y_train: y labels for one batch of train data\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        # TODO: Implement\n",
    "        backprop_tensor = self.__loss.derivative(output, y_train)\n",
    "        for layer in reversed(self.__layers_list):\n",
    "            backprop_tensor = layer.update_weights(backprop_tensor, self.__lr)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417ba3c",
   "metadata": {},
   "source": [
    "## Phase 3: Data Classification\n",
    "In this phase, we will implement and train Feed Forward neural networks with different parameters using the `FeedForwardNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "cfa5f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_pixels_numbers(inp):\n",
    "    r = []\n",
    "    for i in inp:\n",
    "        rr = []\n",
    "        for j in i:\n",
    "            for k in j:\n",
    "                rr.append(k)\n",
    "        r.append(rr)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "287b4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = expand_pixels_numbers(X_train.tolist()), expand_pixels_numbers(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e3b7a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(data=X_train, columns=[\"PixelNo. %i\" % x for x in range(1, 25*25 + 1)])\n",
    "X_test = pd.DataFrame(data=X_test, columns=[\"PixelNo.%i\" % x for x in range(1, 25*25 + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "de72471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.DataFrame(data=Y_train.tolist(), columns=[\"labels\"])\n",
    "Y_test = pd.DataFrame(data=Y_test.tolist(), columns=[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d4969",
   "metadata": {},
   "source": [
    "### Training Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d4dcf4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for building and training a model\n",
    "\n",
    "INPUT_SHAPE = 25*25\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAINLOADER = [batch for batch in Dataloader(X_train, Y_train, n_classes=10, batch_size=BATCH_SIZE, shuffle=False)]\n",
    "TESTLOADER = [batch for batch in Dataloader(X_test, Y_test, n_classes=10, batch_size=BATCH_SIZE, shuffle=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8a17bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "# network.add_layer(45, input_shape=INPUT_SHAPE, activation=LeakyRelu(), weight_initializer='uniform')\n",
    "# network.add_layer(10, activation=LeakyRelu(), weight_initializer='uniform')\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "# network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a06cb8bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-269-1eaff58b510a>:262: RuntimeWarning: invalid value encountered in true_divide\n",
      "  softmax_value /= softmax_value.sum(axis=1)\n",
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: invalid value encountered in multiply\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain: Average Accuracy: 9.879493942946464\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n"
     ]
    }
   ],
   "source": [
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23c848",
   "metadata": {},
   "source": [
    "### Part I: Network training\n",
    "\n",
    "The neural network that I implemented consists of `1` input layer, `1` hidden layer and `1` output layer. `625` input nodes, `45` hidden nodes and `10` output nodes corresponding to `10` labels that we have.\n",
    "\n",
    "I have tried higher number of hidden nodes and it'll result in higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "da5faab5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-269-1eaff58b510a>:262: RuntimeWarning: invalid value encountered in true_divide\n",
      "  softmax_value /= softmax_value.sum(axis=1)\n",
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: invalid value encountered in multiply\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain: Average Accuracy: 9.879493942946464\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n"
     ]
    }
   ],
   "source": [
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "63e65ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 10.13594177413052\tAverage Loss: 2.274236090861463\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.2497816750304755\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 13.599892536146932\tAverage Loss: 2.2116339947414927\n",
      "\tTest: Average Accuracy: 17.997834578804348\tAverage Loss: 2.14593783169617\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 25.962924970691677\tAverage Loss: 2.0441111997798647\n",
      "\tTest: Average Accuracy: 28.236030910326086\tAverage Loss: 1.924262480835608\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 39.95447440406409\tAverage Loss: 1.815911884507805\n",
      "\tTest: Average Accuracy: 42.33738111413044\tAverage Loss: 1.7091154616432582\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 49.041617819460726\tAverage Loss: 1.6221445186851413\n",
      "\tTest: Average Accuracy: 51.01796025815217\tAverage Loss: 1.5415970347724464\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 54.691139116842514\tAverage Loss: 1.4740983370619634\n",
      "\tTest: Average Accuracy: 55.81967561141305\tAverage Loss: 1.417192653936798\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 58.42130715123095\tAverage Loss: 1.3619463442470552\n",
      "\tTest: Average Accuracy: 58.91537873641305\tAverage Loss: 1.3230084279714267\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 60.96448808128175\tAverage Loss: 1.2755617154981327\n",
      "\tTest: Average Accuracy: 61.153617527173914\tAverage Loss: 1.2506917930143895\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 62.80847010550996\tAverage Loss: 1.2091211676532598\n",
      "\tTest: Average Accuracy: 62.799125339673914\tAverage Loss: 1.195360501609601\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 64.20217858538491\tAverage Loss: 1.158280309746139\n",
      "\tTest: Average Accuracy: 64.0393597146739\tAverage Loss: 1.1530042372977585\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 65.25293083235638\tAverage Loss: 1.1191109556376497\n",
      "\tTest: Average Accuracy: 65.1282269021739\tAverage Loss: 1.1201744248787324\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 66.21888432981633\tAverage Loss: 1.088349170213067\n",
      "\tTest: Average Accuracy: 65.95533288043478\tAverage Loss: 1.0941731319919559\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 67.0163149667839\tAverage Loss: 1.0635693753150943\n",
      "\tTest: Average Accuracy: 66.69752038043478\tAverage Loss: 1.0730617806627916\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 67.66476162563501\tAverage Loss: 1.043089237796435\n",
      "\tTest: Average Accuracy: 67.27857506793478\tAverage Loss: 1.0555099951232925\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 68.24081672528331\tAverage Loss: 1.0257744642816093\n",
      "\tTest: Average Accuracy: 67.84498131793478\tAverage Loss: 1.0406166669400625\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = 25*25\n",
    "LEARNING_RATE = 0.00003\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "# network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1c664",
   "metadata": {},
   "source": [
    "### Part II : Network Weighting\n",
    "\n",
    "If zero initial weights are chosen, then the learning rate has no influence on a neuron's predicted class label. If you initialize all weights with zeros then every hidden unit will get zero independent of the input since the weight matrix is all zeros and multiplication will result in zero. So, when all the hidden neurons start with the zero weights, then all of them will follow the same gradient and for this reason starting with zeros affects only the scale of the weight vector, not the direction. Also, having zero weights to start with will prevent the network from learning. The errors backpropagated through the network is proportional to the value of the weights. If all the weights are the same, then the backpropagated errors will be the same, so all of the weights will be updated by the same amount which means the whole process was useless. To avoid this problem, the initial weights of the network should be unequal a way of doing this is to assign random values to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ebf6f482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: invalid value encountered in multiply\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-269-1eaff58b510a>:262: RuntimeWarning: invalid value encountered in true_divide\n",
      "  softmax_value /= softmax_value.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain: Average Accuracy: 9.86117624071903\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='zero')\n",
    "# network.add_layer(40, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='zero')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c262d",
   "metadata": {},
   "source": [
    "### Part III : Impact of learning rate\n",
    "\n",
    "In this section, by decreasing and increasing the `learning rate` parameter, we will train the network and find the optimal value for our network\n",
    "\n",
    "The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "40da32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 9.95398593200469\tAverage Loss: 2.2774711352978065\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.272040572718839\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 10.090758108636185\tAverage Loss: 2.2661612047644035\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.261050516012077\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 10.282483391949981\tAverage Loss: 2.254204774117048\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.2476955347380967\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 10.538931223134037\tAverage Loss: 2.238805752980102\n",
      "\tTest: Average Accuracy: 9.715735394021738\tAverage Loss: 2.2296398835410445\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 10.995652598671356\tAverage Loss: 2.2170932285206777\n",
      "\tTest: Average Accuracy: 9.852454144021738\tAverage Loss: 2.20337413551036\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 13.031701836654943\tAverage Loss: 2.184871050986452\n",
      "\tTest: Average Accuracy: 14.021314538043478\tAverage Loss: 2.1640168321341804\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 19.267731535756155\tAverage Loss: 2.1369819250786803\n",
      "\tTest: Average Accuracy: 19.252717391304348\tAverage Loss: 2.106516919805789\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 26.8857952325127\tAverage Loss: 2.069809151433284\n",
      "\tTest: Average Accuracy: 25.93919836956522\tAverage Loss: 2.029610049765426\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 34.689282923016805\tAverage Loss: 1.985818566787546\n",
      "\tTest: Average Accuracy: 33.06704313858695\tAverage Loss: 1.9399596282233014\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 41.026328644001566\tAverage Loss: 1.894490609390842\n",
      "\tTest: Average Accuracy: 38.94679857336956\tAverage Loss: 1.8487923381684443\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 45.35975967174678\tAverage Loss: 1.8057315799361007\n",
      "\tTest: Average Accuracy: 43.68800951086956\tAverage Loss: 1.763830326198819\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 48.42125830402501\tAverage Loss: 1.7245539059607453\n",
      "\tTest: Average Accuracy: 47.23972486413044\tAverage Loss: 1.687649616969113\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 50.77691481047284\tAverage Loss: 1.6521267870285243\n",
      "\tTest: Average Accuracy: 49.85691236413044\tAverage Loss: 1.620337413658715\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 52.51621727237202\tAverage Loss: 1.5880965333101305\n",
      "\tTest: Average Accuracy: 52.04441236413044\tAverage Loss: 1.5611766844501451\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 54.05036146932395\tAverage Loss: 1.531626742254084\n",
      "\tTest: Average Accuracy: 53.62156080163044\tAverage Loss: 1.5091846304503727\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00001\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e28f6321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 11.766901133255178\tAverage Loss: 2.2596043007486792\n",
      "\tTest: Average Accuracy: 17.983186141304348\tAverage Loss: 2.1815058427636465\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 27.180930050801095\tAverage Loss: 2.021627236313528\n",
      "\tTest: Average Accuracy: 35.70163892663044\tAverage Loss: 1.82325061234299\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 45.39917936694021\tAverage Loss: 1.6667743348707176\n",
      "\tTest: Average Accuracy: 52.18198029891305\tAverage Loss: 1.520542596062058\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 55.00532434544744\tAverage Loss: 1.422904970898598\n",
      "\tTest: Average Accuracy: 57.867484714673914\tAverage Loss: 1.3388313511025711\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 60.56271981242673\tAverage Loss: 1.2707102282258864\n",
      "\tTest: Average Accuracy: 61.422172214673914\tAverage Loss: 1.2246669955596834\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 63.49843688940992\tAverage Loss: 1.1731521757241146\n",
      "\tTest: Average Accuracy: 63.873344089673914\tAverage Loss: 1.1521244389270042\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 65.52647518561939\tAverage Loss: 1.1104385482237635\n",
      "\tTest: Average Accuracy: 65.3333050271739\tAverage Loss: 1.1045194269265255\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 66.75986713559985\tAverage Loss: 1.0681185816733012\n",
      "\tTest: Average Accuracy: 66.55103600543478\tAverage Loss: 1.0711904789334334\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 67.80241305197343\tAverage Loss: 1.0374265088456789\n",
      "\tTest: Average Accuracy: 67.42505944293478\tAverage Loss: 1.046246767722544\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 68.63349941383353\tAverage Loss: 1.0136976752952727\n",
      "\tTest: Average Accuracy: 68.02564538043478\tAverage Loss: 1.0265492727925287\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 69.298700664322\tAverage Loss: 0.9944762059366101\n",
      "\tTest: Average Accuracy: 68.61646569293478\tAverage Loss: 1.0103769350544887\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 69.79083626416569\tAverage Loss: 0.9783977289065777\n",
      "\tTest: Average Accuracy: 69.11939538043478\tAverage Loss: 0.9967348886815811\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 70.27774521297381\tAverage Loss: 0.9646498788757144\n",
      "\tTest: Average Accuracy: 69.43677819293478\tAverage Loss: 0.98501436886524\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 70.65142633841344\tAverage Loss: 0.9527164094527399\n",
      "\tTest: Average Accuracy: 69.77857506793478\tAverage Loss: 0.9748210004138003\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 71.00068386088316\tAverage Loss: 0.9422489886182557\n",
      "\tTest: Average Accuracy: 70.04224694293478\tAverage Loss: 0.9658835065485715\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "de5b02f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate =  7e-05 \n",
      "\n",
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 14.16041422430637\tAverage Loss: 2.2352796942513846\n",
      "\tTest: Average Accuracy: 23.35130774456522\tAverage Loss: 2.0526501576811262\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 38.36571903087143\tAverage Loss: 1.8062897182027855\n",
      "\tTest: Average Accuracy: 51.21815557065217\tAverage Loss: 1.5693651612384105\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 53.94656115670184\tAverage Loss: 1.4377058846219217\n",
      "\tTest: Average Accuracy: 58.24643342391305\tAverage Loss: 1.3200910443714953\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 60.97235248143806\tAverage Loss: 1.2432015082271102\n",
      "\tTest: Average Accuracy: 62.335258152173914\tAverage Loss: 1.1883995462233803\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 64.49912075029309\tAverage Loss: 1.1338904219994506\n",
      "\tTest: Average Accuracy: 64.8206097146739\tAverage Loss: 1.1131513431913427\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 66.65328253223916\tAverage Loss: 1.0699264414612395\n",
      "\tTest: Average Accuracy: 66.6370159646739\tAverage Loss: 1.0671715273856648\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 68.0246678389996\tAverage Loss: 1.0288931687992708\n",
      "\tTest: Average Accuracy: 67.6770550271739\tAverage Loss: 1.035919629069582\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 68.98275693630325\tAverage Loss: 0.9996432770418358\n",
      "\tTest: Average Accuracy: 68.40650475543478\tAverage Loss: 1.0126939770398005\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 69.74321023837436\tAverage Loss: 0.9773106729563938\n",
      "\tTest: Average Accuracy: 69.09498131793478\tAverage Loss: 0.9944769742086429\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 70.30128956623682\tAverage Loss: 0.9595713013420606\n",
      "\tTest: Average Accuracy: 69.55396569293478\tAverage Loss: 0.9797463315863773\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 70.72626025791325\tAverage Loss: 0.9451178725485846\n",
      "\tTest: Average Accuracy: 69.95923913043478\tAverage Loss: 0.9676177769498114\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 71.17931809300508\tAverage Loss: 0.9331176802824055\n",
      "\tTest: Average Accuracy: 70.22482167119566\tAverage Loss: 0.9575099545784364\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 71.5517780382962\tAverage Loss: 0.9230002939370469\n",
      "\tTest: Average Accuracy: 70.54220448369566\tAverage Loss: 0.9490069387816954\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 71.84119773348965\tAverage Loss: 0.9143590837456042\n",
      "\tTest: Average Accuracy: 70.78146229619566\tAverage Loss: 0.9417984400585727\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 72.06345252051582\tAverage Loss: 0.9068974065268564\n",
      "\tTest: Average Accuracy: 70.94747792119566\tAverage Loss: 0.9356495593580096\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00007\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "c09fb5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 16.648642047674873\tAverage Loss: 2.2072149735442728\n",
      "\tTest: Average Accuracy: 32.216372282608695\tAverage Loss: 1.8725964894724392\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 46.10712192262603\tAverage Loss: 1.6017196007231087\n",
      "\tTest: Average Accuracy: 56.375254755434774\tAverage Loss: 1.362076100274282\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 59.71243649863228\tAverage Loss: 1.2563072041275694\n",
      "\tTest: Average Accuracy: 62.420176630434774\tAverage Loss: 1.1758403440818936\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 64.55285267682689\tAverage Loss: 1.118556834898778\n",
      "\tTest: Average Accuracy: 65.37427819293478\tAverage Loss: 1.0985206054763377\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 66.87710042985542\tAverage Loss: 1.053602577306038\n",
      "\tTest: Average Accuracy: 66.63892663043478\tAverage Loss: 1.056985721477193\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 68.20085971082455\tAverage Loss: 1.0144131968458394\n",
      "\tTest: Average Accuracy: 67.62525475543478\tAverage Loss: 1.0297146653277678\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 69.2495115279406\tAverage Loss: 0.987262856169265\n",
      "\tTest: Average Accuracy: 68.22095788043478\tAverage Loss: 1.0098127766905927\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 69.90894880812817\tAverage Loss: 0.967324060941349\n",
      "\tTest: Average Accuracy: 68.75806725543478\tAverage Loss: 0.9948504699755659\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 70.44871043376318\tAverage Loss: 0.9523087627608208\n",
      "\tTest: Average Accuracy: 69.05294667119566\tAverage Loss: 0.9837977096860534\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 70.80895857756936\tAverage Loss: 0.9408482339296819\n",
      "\tTest: Average Accuracy: 69.21896229619566\tAverage Loss: 0.9759941255953197\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 71.03854044548652\tAverage Loss: 0.9320322513184781\n",
      "\tTest: Average Accuracy: 69.45333729619566\tAverage Loss: 0.9709134991978978\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 71.25468933177022\tAverage Loss: 0.9252508945528842\n",
      "\tTest: Average Accuracy: 69.42404042119566\tAverage Loss: 0.9681588251665119\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 71.38501367721766\tAverage Loss: 0.9201033138350385\n",
      "\tTest: Average Accuracy: 69.43571671195653\tAverage Loss: 0.967460444304453\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 71.50591051191871\tAverage Loss: 0.9163290760520117\n",
      "\tTest: Average Accuracy: 69.48454483695653\tAverage Loss: 0.9686510416649293\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 71.60116256350136\tAverage Loss: 0.9137601219637884\n",
      "\tTest: Average Accuracy: 69.42595108695653\tAverage Loss: 0.9716388036337198\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00010\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "992cc745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 20.44597499023056\tAverage Loss: 2.244464124939393\n",
      "\tTest: Average Accuracy: 44.13828974184783\tAverage Loss: 1.6711032378508155\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 45.02828253223915\tAverage Loss: 1.8352425101113234\n",
      "\tTest: Average Accuracy: 53.83555536684783\tAverage Loss: 1.4894289133942311\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 47.0326299335678\tAverage Loss: 2.7143680459786963\n",
      "\tTest: Average Accuracy: 37.31700067934783\tAverage Loss: 3.424777073867182\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 38.78077373974209\tAverage Loss: 11.406613011689743\n",
      "\tTest: Average Accuracy: 28.53685461956522\tAverage Loss: 21.915892753153354\n",
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: invalid value encountered in multiply\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-269-1eaff58b510a>:262: RuntimeWarning: invalid value encountered in true_divide\n",
      "  softmax_value /= softmax_value.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain: Average Accuracy: 27.29596522078937\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00015\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321efb90",
   "metadata": {},
   "source": [
    "By checking multiple values for learning rate, optimal `learning rate` was around $0.00007$\n",
    "\n",
    "**Learning Rate * 0.1**\n",
    "\n",
    "Now, when we multiplied the optimum learning rate by 0.1, the speed of the learning process decreased and it seems that we need more epochs to converge to the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7689d097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate =  7e-06 \n",
      "\n",
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 9.99184251660805\tAverage Loss: 2.277086793710926\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.272934209659802\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 10.083431027745213\tAverage Loss: 2.2681288042990064\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.264816411974413\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 10.166471277842907\tAverage Loss: 2.25956083453256\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.255590184057596\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 10.259280969128566\tAverage Loss: 2.2494896394412818\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: 2.2444184478855465\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 10.460775693630325\tAverage Loss: 2.236963548724847\n",
      "\tTest: Average Accuracy: 9.720618206521738\tAverage Loss: 2.2302082915267554\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 11.011527940601798\tAverage Loss: 2.2207384286192275\n",
      "\tTest: Average Accuracy: 10.550696331521738\tAverage Loss: 2.211544471178462\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 12.997508792497069\tAverage Loss: 2.1992538082333715\n",
      "\tTest: Average Accuracy: 14.358228600543478\tAverage Loss: 2.1867273469862636\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 16.53805197342712\tAverage Loss: 2.1707735944192588\n",
      "\tTest: Average Accuracy: 17.104279891304348\tAverage Loss: 2.1540490191938453\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 19.534828057835092\tAverage Loss: 2.133832122142283\n",
      "\tTest: Average Accuracy: 18.510529891304348\tAverage Loss: 2.11243259680761\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 22.897958186791715\tAverage Loss: 2.0880055484716715\n",
      "\tTest: Average Accuracy: 21.381623641304348\tAverage Loss: 2.0622857244040405\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 27.458186791715512\tAverage Loss: 2.034597541434404\n",
      "\tTest: Average Accuracy: 26.033882472826086\tAverage Loss: 2.005869018874392\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 32.94773348964439\tAverage Loss: 1.9764512128662752\n",
      "\tTest: Average Accuracy: 31.296492866847824\tAverage Loss: 1.9464797243621152\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 37.87289957014459\tAverage Loss: 1.9167436984159667\n",
      "\tTest: Average Accuracy: 35.673403532608695\tAverage Loss: 1.8870243576538832\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 41.65069363032435\tAverage Loss: 1.8578499859481672\n",
      "\tTest: Average Accuracy: 39.67922044836956\tAverage Loss: 1.8293179558966692\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 44.77481438061743\tAverage Loss: 1.801131614840845\n",
      "\tTest: Average Accuracy: 43.12648607336956\tAverage Loss: 1.7742972815014895\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00007 * 0.1\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b307db4",
   "metadata": {},
   "source": [
    "**Learning Rate * 10**\n",
    "\n",
    "Now, when we multiplied the optimum learning rate by 10, we faced with the problem of dying *Relu* and its obvious because weight will increase and it can cuase in vanishing in gradients. We can see that there is a small change between accuracies of different epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "4eb2ebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate =  0.0006999999999999999 \n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: divide by zero encountered in log\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-270-1e27c19a66a1>:34: RuntimeWarning: invalid value encountered in multiply\n",
      "  cross_entropy_value = np.sum(-np.multiply(np.matrix(expected_val, dtype=float), np.log(Softmax()(true_val_mat))), axis=1)\n",
      "<ipython-input-269-1eaff58b510a>:262: RuntimeWarning: invalid value encountered in true_divide\n",
      "  softmax_value /= softmax_value.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain: Average Accuracy: 9.867282141461509\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 9.870945681906996\tAverage Loss: nan\n",
      "\tTest: Average Accuracy: 9.710852581521738\tAverage Loss: nan\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00007 * 10\n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Relu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de00ef6",
   "metadata": {},
   "source": [
    "If the learning rate is low, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny. If the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8658d4",
   "metadata": {},
   "source": [
    "### Part IV :  Impact of activation function\n",
    "\n",
    "In this part, we measure the performance of the network designed in the first part using different *Activation Functions* and compare the results.\n",
    "\n",
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "f55c47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 10.033899960922236\tAverage Loss: 2.3030575701598983\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.3027003872680796\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 10.03267878077374\tAverage Loss: 2.302885904139001\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.30269513092826\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 10.033899960922236\tAverage Loss: 2.302880691532124\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.302689898279914\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 10.036342321219227\tAverage Loss: 2.3028754872074093\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.3026846730290558\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 10.038784681516217\tAverage Loss: 2.302870288127039\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.3026794520371325\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 10.043669402110199\tAverage Loss: 2.302865091189496\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.302674232190302\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 10.050996483001173\tAverage Loss: 2.3028598933138986\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.3026690103927345\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 10.053438843298164\tAverage Loss: 2.3028546914335783\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.3026637835601855\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 10.061987104337632\tAverage Loss: 2.3028494824897408\n",
      "\tTest: Average Accuracy: 10.20210597826087\tAverage Loss: 2.302658548613654\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 10.066871824931614\tAverage Loss: 2.3028442634251944\n",
      "\tTest: Average Accuracy: 10.20698879076087\tAverage Loss: 2.302653302473088\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 10.066871824931614\tAverage Loss: 2.3028390311781104\n",
      "\tTest: Average Accuracy: 10.20698879076087\tAverage Loss: 2.3026480420511133\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 10.0705353653771\tAverage Loss: 2.302833782675798\n",
      "\tTest: Average Accuracy: 10.20698879076087\tAverage Loss: 2.3026427642467615\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 10.080304806565065\tAverage Loss: 2.302828514828454\n",
      "\tTest: Average Accuracy: 10.20698879076087\tAverage Loss: 2.3026374659391693\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 10.09495896834701\tAverage Loss: 2.302823224522875\n",
      "\tTest: Average Accuracy: 10.20698879076087\tAverage Loss: 2.3026321439812114\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 10.096180148495506\tAverage Loss: 2.3028179086160927\n",
      "\tTest: Average Accuracy: 10.20698879076087\tAverage Loss: 2.3026267951930626\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00003\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Sigmoid(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618952b",
   "metadata": {},
   "source": [
    "#### Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1b95f16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 10.151133255177804\tAverage Loss: 2.303093955075309\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.302662290613026\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.302868714632748\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026622587908725\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028686828513965\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026622280006643\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.302868650761749\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.30266219719742\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.302868618657356\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.302662166380864\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.302868586537965\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.302662135550748\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028685544033225\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.302662104706822\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028685222531724\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026620738488375\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.30286849008726\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026620429765443\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028684579053293\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026620120896917\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028684257071252\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026619811880287\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.30286839349239\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026619502713053\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028683612608685\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026619193392697\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028683290123024\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026618883916705\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 10.085189527159047\tAverage Loss: 2.3028682967464347\n",
      "\tTest: Average Accuracy: 10.19722316576087\tAverage Loss: 2.3026618574282547\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=Tanh(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdee884",
   "metadata": {},
   "source": [
    "#### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "305df4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 10.310570535365377\tAverage Loss: 2.2682267676285726\n",
      "\tTest: Average Accuracy: 9.769446331521738\tAverage Loss: 2.238635004703592\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 15.399032825322392\tAverage Loss: 2.1885827719258946\n",
      "\tTest: Average Accuracy: 19.194123641304348\tAverage Loss: 2.1063805712823496\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 28.701152794060178\tAverage Loss: 1.9966498440818374\n",
      "\tTest: Average Accuracy: 30.648140285326086\tAverage Loss: 1.8762818388234983\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 42.1840074247753\tAverage Loss: 1.77403485237334\n",
      "\tTest: Average Accuracy: 45.03651494565217\tAverage Loss: 1.6730451140952762\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 50.62915201250488\tAverage Loss: 1.588829363997444\n",
      "\tTest: Average Accuracy: 52.94178838315217\tAverage Loss: 1.5107791637860857\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 55.744138335287225\tAverage Loss: 1.4442355447753832\n",
      "\tTest: Average Accuracy: 56.70346467391305\tAverage Loss: 1.3897392051155202\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 59.11093200468933\tAverage Loss: 1.3368909825109359\n",
      "\tTest: Average Accuracy: 59.441661005434774\tAverage Loss: 1.3013956416914993\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 61.327715904650255\tAverage Loss: 1.2571518737809133\n",
      "\tTest: Average Accuracy: 61.511973505434774\tAverage Loss: 1.235465377517666\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 63.00297967956233\tAverage Loss: 1.1968645972218914\n",
      "\tTest: Average Accuracy: 62.965140964673914\tAverage Loss: 1.1853093412938627\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 64.33372411098085\tAverage Loss: 1.1506210901532323\n",
      "\tTest: Average Accuracy: 64.2297894021739\tAverage Loss: 1.1465966836615362\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 65.36562133645955\tAverage Loss: 1.1146084299408916\n",
      "\tTest: Average Accuracy: 65.29127038043478\tAverage Loss: 1.1162314679425596\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 66.30070339976554\tAverage Loss: 1.086019451212943\n",
      "\tTest: Average Accuracy: 65.94556725543478\tAverage Loss: 1.0919475721444107\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 67.11278819851505\tAverage Loss: 1.0628168814954009\n",
      "\tTest: Average Accuracy: 66.75123131793478\tAverage Loss: 1.0721163784996128\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 67.74135404454866\tAverage Loss: 1.0435616170644348\n",
      "\tTest: Average Accuracy: 67.32740319293478\tAverage Loss: 1.055587614686336\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 68.2725674091442\tAverage Loss: 1.0272496340522508\n",
      "\tTest: Average Accuracy: 67.73755944293478\tAverage Loss: 1.0415508221780487\n"
     ]
    }
   ],
   "source": [
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=LeakyRelu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa03cd",
   "metadata": {},
   "source": [
    "#### Why Sigmoid and Tanh do not work well?\n",
    "In Tanh and Sigmoid moving towards the ends of the function, y values react very little to the changes in x so the derivative values in these regions are very small and converge to 0. This is called the vanishing gradient and the learning is minimal meaning that it makes the learning process be very slow or even useless. We know that if 0, not any learning. When slow learning occurs, the optimization algorithm that minimizes error can be attached to local minimum values and cannot get maximum performance from the artificial neural network model.\n",
    "\n",
    "So by the explanation above Tanh and Sigmoid will result in vanishing gradient meaning that after a while the learning process will be very slow. So since we are using Stochastic Gradient Descent, these activation functions, Sigmoid and Tanh, won't be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c469ab",
   "metadata": {},
   "source": [
    "#### Why Leaky-Relu is better than Relu?\n",
    "Unlike Relu, LeakyRelu is more balanced and may therefore learn faster. Also LeakyRelu fixes the dying Relu problem, as it doesn’t have zero-slope parts.\n",
    "\n",
    "Dying relu problem: A Relu neuron is dead if it’s stuck in the negative side and always outputs 0. Because the slope of Relu in the negative range is also 0, once a neuron gets negative, it’s unlikely for it to recover. Such neurons are not playing any role in discriminating the input and is essentially useless. Over the time you may end up with a large part of your network doing nothing.\n",
    "\n",
    "Since LeakyRelu assigns a small non zero value for negative parts this will result in solving the problem mentioned above and this is the main advantage of LeakyRelu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818236a",
   "metadata": {},
   "source": [
    "LeakyRelu and Relu results are really close to each other but LeakyRelu result value is a bit higher so at the end I chose LeakyRelu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67564e5e",
   "metadata": {},
   "source": [
    "### Part V : Impact of batch size\n",
    "\n",
    "#### Setting Batch-Size to 16\n",
    "In this part, when we set the `batch-size` to $16$, the speed of learning increased and our model reached to the desirable accuracy faster. It is obvious that the more batches we train our model, the more weight updates are performed and we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e9add823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate =  7e-05 \n",
      "\n",
      "Batch Size =  16 \n",
      "\n",
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 13.786733098866744\tAverage Loss: 2.245679072815379\n",
      "\tTest: Average Accuracy: 20.96361243206522\tAverage Loss: 2.085087051896463\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 37.183616647127785\tAverage Loss: 1.8274579745885546\n",
      "\tTest: Average Accuracy: 50.65769361413044\tAverage Loss: 1.5720876832247668\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 54.13672332942556\tAverage Loss: 1.4327819134194284\n",
      "\tTest: Average Accuracy: 58.311820652173914\tAverage Loss: 1.3117649855105569\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 61.25532434544744\tAverage Loss: 1.234550623080016\n",
      "\tTest: Average Accuracy: 62.506156589673914\tAverage Loss: 1.1817337477028953\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 64.57971864009379\tAverage Loss: 1.1291041395285355\n",
      "\tTest: Average Accuracy: 65.1331097146739\tAverage Loss: 1.1102991442583412\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 66.67037905431809\tAverage Loss: 1.0689174035533346\n",
      "\tTest: Average Accuracy: 66.5686565896739\tAverage Loss: 1.0671727016063322\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 67.97528331379445\tAverage Loss: 1.0303009284853883\n",
      "\tTest: Average Accuracy: 67.5891644021739\tAverage Loss: 1.0379231691415904\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 68.94856389214537\tAverage Loss: 1.0025419129293183\n",
      "\tTest: Average Accuracy: 68.38209069293478\tAverage Loss: 1.0160555991672773\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 69.6763872606487\tAverage Loss: 0.9810188136913051\n",
      "\tTest: Average Accuracy: 68.87037194293478\tAverage Loss: 0.9986119279811259\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 70.24912075029309\tAverage Loss: 0.9635532590765528\n",
      "\tTest: Average Accuracy: 69.39283288043478\tAverage Loss: 0.9841609191346677\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 70.66554318093004\tAverage Loss: 0.9489935041638713\n",
      "\tTest: Average Accuracy: 69.72974694293478\tAverage Loss: 0.971960710870951\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 71.0587631887456\tAverage Loss: 0.9366656351276896\n",
      "\tTest: Average Accuracy: 70.06177819293478\tAverage Loss: 0.9615813297401019\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 71.41256350136773\tAverage Loss: 0.9261292655845543\n",
      "\tTest: Average Accuracy: 70.43287194293478\tAverage Loss: 0.9527285115071198\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 71.71053145760062\tAverage Loss: 0.9170621713213674\n",
      "\tTest: Average Accuracy: 70.64771569293478\tAverage Loss: 0.9451686982548855\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 71.95476748729973\tAverage Loss: 0.9092102081775637\n",
      "\tTest: Average Accuracy: 70.76978600543478\tAverage Loss: 0.9387041142989329\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00007 \n",
    "print(\"Learning Rate = \", LEARNING_RATE, \"\\n\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "print(\"Batch Size = \", BATCH_SIZE, \"\\n\")\n",
    "\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=LeakyRelu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8895eef",
   "metadata": {},
   "source": [
    "#### Setting Batch-Size to 256\n",
    "In this part, when we set the `batch-size` to $256$, the speed of learning decreased. So, it is better to use smaller batch size to make the process of learning faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ea051e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size =  256 \n",
      "\n",
      "Epoch 1:\n",
      "\tTrain: Average Accuracy: 14.375341930441579\tAverage Loss: 2.2364712153980393\n",
      "\tTest: Average Accuracy: 20.67552649456522\tAverage Loss: 2.060876413380826\n",
      "Epoch 2:\n",
      "\tTrain: Average Accuracy: 36.46976357952325\tAverage Loss: 1.8331498464540321\n",
      "\tTest: Average Accuracy: 50.400815217391305\tAverage Loss: 1.5918061903399583\n",
      "Epoch 3:\n",
      "\tTrain: Average Accuracy: 54.08177022274326\tAverage Loss: 1.4398432884431651\n",
      "\tTest: Average Accuracy: 58.59120244565217\tAverage Loss: 1.3117225131075596\n",
      "Epoch 4:\n",
      "\tTrain: Average Accuracy: 61.379005470887066\tAverage Loss: 1.231526366510031\n",
      "\tTest: Average Accuracy: 62.721000339673914\tAverage Loss: 1.1780755139912042\n",
      "Epoch 5:\n",
      "\tTrain: Average Accuracy: 64.73358733880423\tAverage Loss: 1.1253586766181276\n",
      "\tTest: Average Accuracy: 65.1477581521739\tAverage Loss: 1.1079904898975617\n",
      "Epoch 6:\n",
      "\tTrain: Average Accuracy: 66.59955060570536\tAverage Loss: 1.0660710336352712\n",
      "\tTest: Average Accuracy: 66.5881878396739\tAverage Loss: 1.0658627533204335\n",
      "Epoch 7:\n",
      "\tTrain: Average Accuracy: 67.87602579132474\tAverage Loss: 1.0279444304459944\n",
      "\tTest: Average Accuracy: 67.55689538043478\tAverage Loss: 1.0369890696569963\n",
      "Epoch 8:\n",
      "\tTrain: Average Accuracy: 68.889605314576\tAverage Loss: 1.000421692018505\n",
      "\tTest: Average Accuracy: 68.38209069293478\tAverage Loss: 1.0152649380222845\n",
      "Epoch 9:\n",
      "\tTrain: Average Accuracy: 69.66749706916764\tAverage Loss: 0.9790815706713922\n",
      "\tTest: Average Accuracy: 68.85084069293478\tAverage Loss: 0.9979202269102959\n",
      "Epoch 10:\n",
      "\tTrain: Average Accuracy: 70.2301191871825\tAverage Loss: 0.9618117476035734\n",
      "\tTest: Average Accuracy: 69.34888756793478\tAverage Loss: 0.9835840110932992\n",
      "Epoch 11:\n",
      "\tTrain: Average Accuracy: 70.68439820242283\tAverage Loss: 0.9474714237494636\n",
      "\tTest: Average Accuracy: 69.68580163043478\tAverage Loss: 0.9715206064805685\n",
      "Epoch 12:\n",
      "\tTrain: Average Accuracy: 71.08860883157483\tAverage Loss: 0.9353762296217017\n",
      "\tTest: Average Accuracy: 70.08130944293478\tAverage Loss: 0.9612865904991577\n",
      "Epoch 13:\n",
      "\tTrain: Average Accuracy: 71.46595349745995\tAverage Loss: 0.9250686610044339\n",
      "\tTest: Average Accuracy: 70.39380944293478\tAverage Loss: 0.9525717222031774\n",
      "Epoch 14:\n",
      "\tTrain: Average Accuracy: 71.74316139116843\tAverage Loss: 0.916212341241841\n",
      "\tTest: Average Accuracy: 70.61353600543478\tAverage Loss: 0.9451321400665925\n",
      "Epoch 15:\n",
      "\tTrain: Average Accuracy: 71.96419499804611\tAverage Loss: 0.9085458111866165\n",
      "\tTest: Average Accuracy: 70.74239979619566\tAverage Loss: 0.9387669885623003\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "print(\"Batch Size = \", BATCH_SIZE, \"\\n\")\n",
    "\n",
    "network = FeedForwardNN(INPUT_SHAPE)\n",
    "network.add_layer(45, input_shape=INPUT_SHAPE, activation=LeakyRelu(), weight_initializer='uniform')\n",
    "network.add_layer(10, activation=Identical(), weight_initializer='uniform')\n",
    "network.set_training_param(loss=CrossEntropy(), lr=LEARNING_RATE)\n",
    "log = network.fit(EPOCHS, TRAINLOADER, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473fd467",
   "metadata": {},
   "source": [
    "#### What is the reason for using batch in the training process?\n",
    "The batch size defines the number of samples that will be propagated through the network.\n",
    "\n",
    "* It requires less memory. As we train our network using fewer samples, the overall training process requires less memory. This is very important when we are not able to fit the whole dataset in our machine's memory.\n",
    "\n",
    "* Typically networks train faster with mini-batches, because we update the weights after each propagation. If we used all samples during propagation, we would make only 1 update for the network's parameter and this can lead to underfitting.\n",
    "\n",
    "#### What is the problem with extremely small batches?\n",
    "\n",
    "There is no doubt that the smaller the batch is the less accurate the estimation of the gradient will be. If we use extremely small batch size, we would lose the effectiveness of vectorization. Moreover, It will produce a noisier gradient descent. As a result, we prefer to use bigger batch size.\n",
    "\n",
    "**Advantages** of using a batch size instead of number of all samples are for instance choosing a batch size requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory. Typically networks train faster with batches. That's because we update the weights after each propagation. If we used all samples during propagation we would make only one update for the network's parameter meaning that the update won't occur frequently.\n",
    "\n",
    "So the main advantage of dividing the whole dataset into batches and train the network with these batches is that parameter updates will happen frequently so the training process will be better also having batches will require less computation power and memory. Also by using batches and incorporating parallel computing the learning process can be much faster but if we use the whole dataset we can't use parallel computing to our advantage.\n",
    "\n",
    "The smaller the batch the less accurate the estimate of the gradient will be. This means that parameter updates won't be reliable and aren't the best possible updates so the learning process won't be great. On the other hand, smaller batches can lead to better regularization and they'll make the learning process faster and also less memory is needed at each run. Small batch training has been shown to provide improved generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bab41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
